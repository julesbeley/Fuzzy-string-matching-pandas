{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc8a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "\n",
    "text = 'In approximate string matching, the objective is to find matches for short strings \\\n",
    "in many longer texts, in situations where a small number of differences is to be expected. \\\n",
    "The short strings could come from a dictionary, for instance. Here, one of the strings is\\\n",
    "typically short, while the other is arbitrarily long. This has a wide range of applications,\\\n",
    "for instance, spell checkers, correction systems for optical character recognition, and \\\n",
    "software to assist natural language translation based on translation memory.The Levenshtein\\\n",
    "distance can also be computed between two longer strings, but the cost to compute it, which\\\n",
    "is roughly proportional to the product of the two string lengths, makes this impractical. \\\n",
    "Thus, when used to aid in fuzzy string searching in applications such as record linkage, \\\n",
    "the compared strings are usually short to help improve speed of comparisons.[citation needed]\\\n",
    "In linguistics, the Levenshtein distance is used as a metric to quantify the linguistic distance,\\\n",
    "or how different two languages are from one another.[3] It is related to mutual intelligibility,\\\n",
    "the higher the linguistic distance, the lower the mutual intelligibility, and the lower the \\\n",
    "linguistic distance, the higher the mutual intelligibility.'\n",
    "\n",
    "translator = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "text = set(text.translate(translator).split())\n",
    "\n",
    "unique_strings = []\n",
    "\n",
    "for i in range(100):\n",
    "    length = random.randint(1,3)\n",
    "    unique_strings.append(' '.join(random.sample(text,length)))\n",
    "\n",
    "all_letters = list('abcdefghijklmnopqrstuvwxyz')\n",
    "new_strings = []\n",
    "\n",
    "for string in random.choices(unique_strings,k=300):\n",
    "    n_changes = random.randint(0,3)\n",
    "    new_string = list(string)\n",
    "    if len(new_string) > 1:\n",
    "        for change in range(n_changes):\n",
    "            to_change_index = random.randint(1,len(new_string)-1)\n",
    "            replace_by = random.sample(all_letters,1)[0]\n",
    "            new_string[to_change_index] = replace_by\n",
    "    new_strings.append(''.join(new_string))\n",
    "    \n",
    "all_strings = unique_strings + new_strings\n",
    "random.shuffle(all_strings)\n",
    "        \n",
    "test_df = pd.DataFrame(all_strings,columns=['strings'])\n",
    "test_df['key'] = test_df.index\n",
    "\n",
    "test_df = test_df[['key','strings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f71926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import ratio\n",
    "\n",
    "\n",
    "def fuzzy_match(df,key,strings,threshold):\n",
    "    \n",
    "    dictionary = dict(zip(df[key],df[strings]))\n",
    "    unique_keys = [df[key][0]]\n",
    "    duplicates = {key:[] for key in dictionary}\n",
    "\n",
    "    for key in dictionary:\n",
    "        results = []\n",
    "\n",
    "        for unique_key in unique_keys:\n",
    "            to_test = dictionary[key],dictionary[unique_key]\n",
    "            results.append(ratio(*to_test))\n",
    "\n",
    "        results = dict(zip(unique_keys,results))  \n",
    "        original = [key for key,value in results.items() if value >= threshold]\n",
    "\n",
    "        if len(original) == 0:\n",
    "            unique_keys.append(key)\n",
    "\n",
    "        else:\n",
    "            true_original = original[0]\n",
    "            if len(original) > 1:\n",
    "                for other_original in original[1:]:\n",
    "                    unique_keys.remove(other_original)\n",
    "                    duplicates[true_original].append(other_original)\n",
    "                    duplicates[true_original].extend(duplicates[other_original])\n",
    "                    duplicates[other_original] = []\n",
    "                    \n",
    "    return(unique_keys,duplicates)\n",
    "\n",
    "\n",
    "def drop_fuzzy_duplicates(df,key,string,threshold):\n",
    "\n",
    "    unique_keys,duplicates = fuzzy_match(df,key,string,threshold)\n",
    "    \n",
    "    return(df[df['key'].isin(unique_keys)])\n",
    "\n",
    "\n",
    "def fuzzy_duplicated(df,key,string,threshold):\n",
    "    \n",
    "    unique_keys,duplicates = fuzzy_match(df,key,string,threshold)\n",
    "    \n",
    "    return(~df['key'].isin(unique_keys))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
